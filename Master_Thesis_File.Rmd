---
title: "Time Variation of Regression Coefficients related to Macroeconomic News affecting Currency Prices"
author: "Victor Mitchell"
bibliography: ["citations.bib"]
date: "November 5, 2019"
always_allow_html: no
output:
  bookdown::pdf_document2:
    toc: true
    toc_depth: 2
    fig.caption : yes
---
```{r loadlib, results = 'hide', echo = F, message = F, warning = F}
library(bookdown)
library(quantmod)
library(plotly)
library(jtools)
library(knitr)
library(kableExtra)
library(dplyr)
library(ggplot2)
library(ggthemes)
#library(phantomjs)
setwd("~/R tests/finance related projects")
#price_data_usdcad <- read.delim("~/R tests/finance related projects/USDCAD_M1_201901020600_201911051703.csv")
# cadcpi_m_m <- read.delim("~/R tests/finance related projects/cadcpi_m_m.txt")
# cadcpi_newcol <- read.csv("~/R tests/finance related projects/cadcpi_dataframe.csv")
# usdcad_ticks <- read.delim("~/R tests/finance related projects/USDCAD_201906191500_201906191559_ticks.csv")
source("brownian_plot.R")
source("magnitudes_WAR.R")
source("random_walks.R")
newsintro <- read.delim("~/R tests/finance related projects/newsintro.txt")
results <- read.delim("~/R tests/finance related projects/result_table_1.txt")
results_sim <- read.delim("~/R tests/finance related projects/result_table_sim.txt")
qLL_table <- read.delim("~/R tests/finance related projects/qLL_crit.txt")
qLL_results <- read.delim("~/R tests/finance related projects/qLL_results.txt")
colnames(qLL_table) <- c("k",seq(from = 1,to = 5, by = 1))
```

\begin{center}
abstract here when I'm done with the writing and have all results.
\end{center}

# Introduction

Some macroeconomic figures used to gauge the health of a nation's economy are released to the public on a predetermined schedule. These include for example the Non-Farm Employment change that is released on the first friday of every month informing economists and investors alike of the status of employment in the United States.  
Standard economic theory helps us understand that an increase of interest rates is warranted when economies are performing well and prices are generally increasing. Those who decide to increase national interest rates, the central banks, typically refer to measures of inflation in order to make their decisions. Because of this, investors and traders alike pay close attention to news releases (such as inflation, and also the Non-Farm-Payrolls in the case of the United States) and react according to the results. These news releases are not made public until specific times on specific days and since investors and traders react to the same news the moment it is released, the result is often a violent reaction of price in one direction or another. The common discourse is that the direction and the magnitude of the change of price depends on the difference between the expectation of the market (combined expecation of worldwide investors) and the result of the news release.  
In this paper, we decide to use currency pairs to measure the price shocks. As certain news pertaining to a particular country affects its respective currency more than other ones, it makes sense to observe the currency most relevant to the news announcement. As currency prices are typically measured in pairs, the second chosen currency will be another major currency that is known for its high liquidity (EUR, USD or CHF) and does not have any other news announcements at the same time^[When simultaneous news cannot be avoided, a sequence of  stability tests will be applied to ensure time variation is identified on a specific news release]. As an example, we would use the USD/CAD currency pair to measure the effect of the Canadian Consumer Price Index (CPI).  
The paper of [@andersen_micro_2003] reveals that over the time period between 1987 and 2002 there has been little time-variation in the reaction to news. Some more recently published literature of [@ben_omrane_time-varying_2019] involving an analysis on euro-dollar contracts has determined that unlike the the decade(s) encompassing the "Great Moderation" where there was lower relative volatility in the financial markets, the time period between 2004 and 2014 is characterized by evolving reactions to macroeconomic news.  
The objective of this paper is two-fold. Firstly, we aim to employ existing methodologies on new data to identify whether or not instability of market reaction is present. When reactions do change over time, we attempt to estimate the path that it takes using different procedures suggested by the literature given that the instability has been identified. 

  
# Data
  
The minute-by-minute OHLC Data of 7 currency pairs were collected from the Metatrader5 platform between 2008 and the end of 2019. This represents over 4 million data points for each currency pair cosnsidered in the study. Only a small fraction of this data is actually used since we consider only the 5 time frame from when each piece of monthly or quarterly news is released until 5 minutes afterwards. In regards to the collection of macroeconomic figures data, the expectation numbers from the ForexFactory website were used^[The expectations of most online sources such as "Investing.com" or "DailyFX" are very similar. Unfortunately, the aggregation methods are not disclosed to the public.] The process of combining of the two separate datasets was complex and involved careful consideration of erratic scheduling changes of the organizations releasing the figures as well as the daylight savings and timezones of all considered data. 
  
![Minute-by-minute candlechart of the USD/CAD asset on the 19th of June between 15h00 and 16h00 GMT+2. An example of of the sudden price change that can occur during one of the news releases. Green/Red candles represent an increase/decrease in price. The gray indicator is amount of trading activity taking place (a rough guideline as it only includes transactions from that particular brokerage).](candlestick2.png)
  
```{r newintro, echo = FALSE}
kable(newsintro, booktabs = T, caption ="Summary of the news figures considered in the study",
      format = "latex") %>%
kable_styling(latex_options = c("striped", "scale_down")) %>%
row_spec(c(1,10), bold = T, color = "black") %>%
row_spec(c(11,12,13), color = "black",background = "#b3e0ff") %>%
row_spec(c(14,15), color = "black",background = "#c2f0c2") %>%  
row_spec(c(16,17), color = "black",background = "#ffd1b3") 

```

  
# Building blocks of the models {#bb}
  
## The Stable Linear Model that is Time Invariant
  
Being consistent with previous literature on the subject, the first step involves estimating the impact that each piece of news has on its respective currency assuming 1.) That the news effects are constant over time. 2.) The surprise element $S_t$ of the regression is evaluated as:  

\begin{equation}
S_t = \frac{A_t - E_t}{\sigma_d}
\end{equation}
  
$A_t$ is the actual result of the news at time $t$, $E_t$ is the expected result aggregated from experts and $\sigma_d$ is the empirical standard deviation of this difference over the entire sample.  Thereafter, we use this surprise element in a first simple OLS model.  

\begin{equation}
R_t = \beta_0 + \beta_1 S_t + \varepsilon_t
(\#eq:stable)
\end{equation}  
  
With $R_t$ as the 5-minute currency return result and $\varepsilon_t$ is the usual assumed normally distributed error. Moreover, because we are working with a dataset where subsequent observations are suspected to be related to one another, one could expect that the errors of the basic model above be autocorrelated.  Specifically, adjacent $R_t$ would be more similar to one another than reactions that are separated in time to a greater extent. In this case, the inference on the $\beta_1$ would be flawed. Previous researchers have used what is called Heteroskedasticity and Autocorrelation-Consistent (HAC) estimators for the variance of the OLS estimator $\beta_1$. Using the Newey-West estimator for this variance from @newey_simple_1987, we use modified standard errors of the $\beta_1$ in our results. If we are wrong in our assumption in some of the news instances, and there is no underlying autocorrelation of the $R_t$ observations, our estimation of $\beta_1$ is less efficient than the original estimator in those cases. Nonetheless, it remains consistent and ensures we avoid type 1 error of rejecting a true null hypothesis suggesting $\beta_1 = 0$.  
  
```{r tablereg, echo = FALSE}
kable(results, booktabs = T, caption ="Results of regressions tests - HAC standard errors ",
      format = "latex") %>%
kable_styling(latex_options = c("striped")) %>%
row_spec(c(1,10), bold = T, color = "black") %>%
row_spec(c(11,12,13), color = "black",background = "#b3e0ff") %>%
row_spec(c(14,15), color = "black",background = "#c2f0c2") %>%  
row_spec(c(16,17), color = "black",background = "#ffd1b3") %>% 
footnote(general = "The result of OLS estimation, referred to as the 'time invariant' or 'stable' case is presented in this table. The standard errors of the estimator as well as the Newey-West corrected standard errors are also included. The *,**,*** are for 10%, 5% and 1% significance levels respectively."
         ,footnote_as_chunk = T
         ,threeparttable = T)
```
  
Table \@ref(tab:tablereg) shows the result of the different $\beta_1$ coefficients for separate news reports. The construction of the truncation parameter in the Newey-West estimator is such that our monthly news reports consider 2 autocorrelation coefficients whereas the quarterly ones only contain 1. This is due to the difference in the number of observations in our data. A higher estimated autocorrelation between the errors of the regression will result in a stronger correction of the variance of $\beta_1$. In all of the news in Table \@ref(tab:tablereg), the higher standard error does not affect the significance of the term. While it is not formal evidence, we suspect that the regressions where the HAC standard error is larger than its unedited counterpart contain unknown regressors that may come from any of the findings that are well elaborated in other literature such as in @goldberg_time_2013. Overall, these results and that of the Durbin-Watson test show that there is little evidence for significant autocorrelation in the residuals.

\begin{equation}
R_t = \beta_0 + \beta_{1,t} S_t + \varepsilon_t
\end{equation}
  
## The Gaussian Random Walk {#grw}
  
The methods discussed in this paper use Gaussian Random Walks (GRW) extensively. This section serves to underline the features of these processes which will later be essential to the understanding of tests and path estimations. Furthermore, it hopefully helps one fully appreciate the assumptions that will be made when using them and how these processes can be used to help decipher the variation of the $\beta$.
  
The GRW is a sequence of $i.i.d$ random variables where one realization at time $t$ follows the distribution:
  
\begin{equation}
X_t \sim \mathcal{N}(\mu_t,\sigma_t^2)
\end{equation}
  
Using the independence assumption of normally distributed random variables, their sum and the distribution of their sum is then given by \@ref(fig:warplot). 
  
\begin{equation}
U = \sum_{t=1}^{T}X_t  \quad 
U \sim \mathcal{N}(\mu_tT,\sigma_t^2T)
\end{equation}
  
```{r randomwalk, echo = FALSE, fig.cap="Hypothetical Paths of the Beta"}
plotrw()
```
   
In theory the path that the $\beta$ could take could resemble any of the paths in Figure \@ref(fig:randomwalk). The process depicted by the Y2 is a baseline case where the parameter would not vary in time. The Y3 process is a random walk with a $\mu$ of 0 and a fixed $\sigma$ which is in reality the same as white noise. The Y1 is a random walk with a constant negative $\mu$. Finally, the Y4 process depicts a scenario with one significant break in the path of $\beta$ and is the combination of 2 separate random walks with separate constant $\mu$ and $\sigma$. Within the context of our problem, it is important to consider all of these possibilities as the potential evolution in the reactions could be erratic in nature (frequent and haphazard change), or gradual (slow and constant change) over time. We would like to detect the changes regardless of the scenario. 

# Testing for instability of the news impact parameter
  
```{r qLLresults, echo = FALSE}
kable(qLL_results, booktabs = T, caption ="Instability Test Results",
      format = "latex") %>%
kable_styling(latex_options = c("striped")) %>%
row_spec(c(1,10), bold = T, color = "black") %>%
row_spec(c(11), color = "black",background = "#b3e0ff") %>%
row_spec(c(17), color = "black",background = "#c2f0c2") %>%
row_spec(c(19), color = "black",background = "#ffd1b3") %>%
footnote(general = "Results of the three instability tests performed for each piece of macroeconomic news. Multiple tests were performed for some news that occured simultaneously because the tests are able to detect instability but not pin-point the exact source of instability for more than 1 figure. The n.s,*,**,*** are for non-significant, 10%, 5% and 1% levels of significance respectively."
         ,footnote_as_chunk = T
         ,threeparttable = T) 
```
  
  
## The quasi-Local-Level Test {#qLLtest}
  
```{r qLL, echo = FALSE}
kable(qLL_table, booktabs = T, caption ="Asymptotic Critical Values of the qLL Statistic",
      format = "latex") %>%
kable_styling(latex_options = c("striped")) %>%
footnote(general = "Extract of the critical values of the qLL Statistic. k represents the number of potential unstable coefficients (number of parameters in the model) whereas 1%, 5% and 10% are the significance levels where a lower value is stronger evidence that instability is present."
         ,footnote_as_chunk = T
         ,threeparttable = T) 
```
  
There exists many ways to test whether $\beta_t$ is time dependent or not. We first choose the methodology of the authors of [@elliott_efficient_2006] and replicate their method. The advantage of their test is that it identifies instability no matter whether it comes in the form of a single break, many breaks, or a continuous change (all of which are feasibly possible in the context of our investigation). Furthermore, the result of the Monte Carlo simulations over different processes also showed that compared to  other tests that exist, the qLL test has the most power in smaller samples.

The quasi-Local-Level (qLL) test enables one to test for many different types of persistent processes of the $\beta_t$. It is explained that many of these breaking processes can have a "temporary memory" (strictly speaking are strongly mixing) but will be well approximated by a Wiener process.^[Theorem 7.30 of [@white_asymptotic_2001] can be applied since certain assumptions are made about the process]. This is extremely practical in our scenario as there are many possiblities for the possible variation of the $\beta_t$. The Null Hypothesis implies there is a stable parameter as in a familiar OLS regression. We obtain the likelihood under the Null assuming that the $R_t$ observations are independently and identically distributed (and therefore so are their first differences):  
  
\begin{equation}
L_{H0}(\beta_0, \beta_1, \sigma^2) = log\prod_{t=1}^{T} p(\Delta R_t | S_t ; \beta_0, \beta_1, \sigma^2)
(\#eq:LH0)
\end{equation}

\begin{equation}
= -\frac{T}{2}log(2\pi) - Tlog(\sigma) - \frac{1}{2\sigma^2}\sum_{t=1}^{T}(\Delta R_t - (\Delta\beta_0 + \Delta\beta_1 S_t))^2
\end{equation}

Only the last term of \@ref(eq:Lh0) is kept as the first constants will cancel out. $\Delta\beta_0 + \Delta\beta_1 S_t$ becomes 0 as the terms do not change with time.

\begin{equation}
L_{H0} = \frac{1}{2\sigma^2}\sum_{t=1}^{T}(\Delta R_t)^2
\end{equation}

This contrasts with the alternative where instability is implied. We assume $\beta_t - \beta_0$ is approximated by the Gaussian random walk and $\Delta R_t$ is therefore a Gaussian moving average of order 1 MA(1) with the specification: $\Delta R_t \sim \eta_t + \psi_\eta \eta_{t-1}$, $\eta_t \sim iidN(0,\sigma^2_\eta)$, constant $\psi_\eta < 1$. Using the same $i.i.d$ assumption we obtain the likelihood of this alternative process:

\begin{equation}
L_{HA} = \frac{1}{2\sigma^2}\sum_{t=1}^{T}\eta^2
\end{equation}

The qLL statistic is obtained by subtracting $L_{HA}$ from $L_{H0}$: $\frac{\sigma^2}{\sigma^2}\sum_{t=1}^{T}\eta^2 - \sum_{t=1}^{T}(\Delta R_t)^2$. The test is therefore a variant of the Likelihood Ratio Test ($LR_T$), so while it does not follow a chi-square distribution exactly, it does follow a certain related distribution that has its percentiles defined by @elliott_efficient_2006 and reported in their table, reproduced here as Table \@ref(tab:qLL). The general extension to the $LR_T$ can be made where we can reject the model related to the Null Hypothesis (the stable model) if the critical value is sufficiently negative. To obtain the $\eta$ term it is necessary to follow additional matrix algebra and use the result of regressions [@elliott_efficient_2006]. The appropriate steps are available in Appendix [Section 9.2](#stepsqLL). While it is true that the underlying process can follow many different kinds of processes that could be very complex, we can detect their presence if we allow for their path to be approximated by a GRW (equivalent to having their first difference that follows an MA(1)).
  

  
## CUSUM and CUSUM-squared tests

Separate tests to the one explained previously are considered. We explore the ones elaborated in @brown_techniques_1975 named the CUSUM and CUSUM-squared. These tests use the successive error terms of predictions of a standard Recursive-Least-Squares (RLS) model (details for this model in [Section 9.2](#stepsslrs)) that assumes stability of the $\beta$ parameter [@young_recursive_2011]. In this specific application, we use the result of the standard OLS as a baseline *prior* or initial value for the algorithm (ie. $\beta_0$) and we use an empirical $\hat{\sigma}_\varepsilon$ residual error that is based on the entire sample. By examining the prediction errors, one can observe whether or not they violate the $N(0,\sigma^2)$ assumptions. Namely that they are a 1.) zero mean sequence $E(\varepsilon_t) = 0$ and 2.) that they are serially uncorrelated $E(\varepsilon_t \varepsilon_j) = 0$ when $t \neq j$. The first step consists in running a standard RLS algorithm and obtaining one-step-ahead errors from it. These errors are obtained as follows:
  
\begin{equation}
u_t = R_t - E(\hat{R_t}|R_{t-1})
\end{equation}
  
Effectively the realized price shock minus the predicted price shock at each observation.  A transformation of these errors enables us to obtain a homoscedastic series.
  
\begin{equation}
u_{n,t} = \frac{u_t}{(1 + S_t^T P_t S_t)^{0.5}}
\end{equation}
  
Where $P_t$ is the covariance (or variance if there is only one news element at a time) of the $\beta_t$. A new series of summed up and standardized for use in the CUSUM test.  

\begin{equation}
W_t = \frac{1}{\hat\sigma_{cs}}\sum_{i=k+1}^{t} u_{n,i}
\end{equation}
  
In theory, these successively compounded errors should not stray too far from the zero-line if the true $\beta_t$ is constant. We also use the confidence bands suggested by @brown_techniques_1975. They are constructed by constructing pairs of lines starting at time *k*: $\pm a(T-k)^{0.5}$ and ending at time *T*: $\pm 3a(T-k)^{0.5}$
  
```{r, echo = FALSE}
plotcusum(read.csv("~/R tests/finance related projects/cusum_df.csv"),
          read.csv("~/R tests/finance related projects/points_df.csv"))
```

  
The second test, the CUSUM-squared can be run by once more by creating a new series, similar to the $W_t$ from earlier. Here we consider:
  
\begin{equation}
V_t = \frac{\sum_{i=k+1}^{t} u_{n,i}}{\sum_{i=k+1}^{T} u_{n,i}}
\end{equation}
  
With a Null-Hypothesis of a constant parameter(s), these cumulative sum of squares should follow a beta distribution and its mean should be $(k-h)/(N-h)$. As significance levels, we use the bounds set as: $\pm c_o + (k-h)/(N-h)$. The values of $c_0$ depend on the sample size. Our sample sizes are typically larger than the maximum value given in @brown_techniques_1975. For those cases where
  
```{r, echo = FALSE}
plotcusumsq(read.csv("~/R tests/finance related projects/cusum_sq_df.csv"))
```

# Parameter Path Estimations

Having established that there is instability over time in the market reactions to at least some of the news, we take on the task of obtaining a time series that conveys the change over the timeframe considered. Specifically, we would like to obtain a visual representation of the change of the parameter such that a researcher or investor is able to easily gather information from.
  
Two different algorithms from literature are utilized and both make different assumptions. Both are useful to help understand the possible sources of instability, to compare the different announcements and to make short-term forecasts. However, the main fundamental difference is that the WAR estimation uses the entire sample while the "simpler" STVP will only use outcomes that occured in the past for its estimation.
  
## Weighted Average Risk Minimization
  
Conveniently, this method is somewhat of a natural extended use of the qLL test elaborated in [Section 4.1](#qLLtest). It provides a heuristic means means to approximate a parameter path [@muller_efficient_2010]. 
  
First, we consider our stable model which is analogous to the linear regression model that is presented in the Equation \@ref(eq:stable). The resulting $\beta_{MLE/OLS}$ parameter obtained either through maximum likelihood estimation or minimization of ordinary least squares (depicted as the dot-dashed line in Figure \@ref(fig:warplot)) is representative of a scenario where it is assumed there is no time-related evolution of market reaction to news. As seen before, the likelihood in this case can be developed as Equation \@ref(eq:LH0) or also written more generally as $\sum_{t=1}^{T}l_t (\theta)$ with $\theta$ containing the constant parameters.
  
We then turn to the case of varying $\beta_t$. Here, the general likelihood is the same but with time varying parameter(s) contained in $\theta_t$ = $\theta + \delta_t$ in $t = 1,...,T$. The $\delta_t$ can be imagined as the vertical distances in Figure \@ref(fig:warplot)) between the constant $\beta$ case and the *true* parameter path at time *t* that is unknown and that we are attempting to approximate. The main argument for the method is that this general likelihood function for a time varying model can be approximated by a second-order Taylor expansion of the likelihood function around $\beta_{MLE}$ and as a result an approximate estimate of the $\delta_t$ term mentioned earlier is obtainable. Effectively it is demonstrated in the paper that the approximation of the log-likelihood function of the parameter path can be restructured such that a log-likelihood function of a gaussian random variable is recognizable and results in a "pseudo model" as:  
  
\begin{equation}
\beta_{MLE} = \beta_t + T^{-1/2}\hat{H}^{-1}v_0
\end{equation}
  
\begin{equation}
s_t(\beta) = \hat{H}\delta_t + v_t, t = 1,...,T
\end{equation}

$\hat{H}$ is the Hessian of the Taylor approximation divided by the sample size T, $s_t$ is the score function for the stable model. The exact derivations to obtain (15) are elaborated in @muller_efficient_2010. The $\delta_t$ vertical distances can be isolated in the latter expression as all the other elements are available to us. 
  
Essentially, the score function $s_t$ and Hessian $\hat{H}$ can be extracted from the stable linear regression shown in section [Section 2](#bb) and the likelihood function is therefore the one seen in equation (7). Deriving once and then twice with respect to $\beta_1$ yields the following respectively:
  
\begin{equation}
s_t = \frac{dl}{d\beta_1} = -\sigma^{-2} \sum_{t=1}^T (R_t - \hat{\beta}_0 - \hat{\beta}_{1}S_t)S_t
(\#eq:stlr)
\end{equation}
  
\begin{equation}
\hat{H} = \frac{dl^2}{d^2\beta_1^T} = -\sigma^{-2}\sum_{t=1}^T S_t^2
(\#eq:helr)
\end{equation}
  
These two estimations, alongside the original $\beta_1$ and $\sigma^2$ from the time-invariant model are all that is needed to apply the entire WAR algorithm. The $s_t$ and $\hat{H}$ provide a direction that the parameter can take. However, the magnitude of these changes from one step to the next still needs to be explained.  
  
In Figure \@ref(fig:warplot), one can observe 11 different random walk functions as well as the final estimated path for the UK CPI example. While they are all constructed using the Score and Hessian seen in \@ref(eq:stlr) and \@ref(eq:helr), they each have a unique end point standard deviation. The weights in step (e) of the Appendix [Section 9.3](#stepswar) depend on the sample size and the qLL statistic that are obtained using the qLL test statistic from [Section 4.1](#qLLtest). Specifically, it can be drawn from the expression $\exp^{-\frac{1}{2}qLL(c_i)}$ that a higher standard deviation in a RW function leads to a smaller qLL statistic and therefore more importance is placed on the weight associated with that particular RW. However, this is balanced by the preceding expression $\sqrt{T(1-r_i^2)r_i^{T-1}/(1-r_i^{2T})}$ that has the opposite effect of simply placing more weight on smaller RW functions. In our example, the random walk function that contributes the most weight for the UK CPI case is the 4th one.

```{r warplot, echo = FALSE, fig.cap="WAR minimizing over the 11 different randow walk weighting functions"}
plotwar(read.csv("~/R tests/finance related projects/gbpcpi_path.csv"),
        read.csv("~/R tests/finance related projects/gbpcpi_magn.csv"),
        "WAR minimizing over the 11 different randow walk weighting functions")
```

## Standard Recursive Time Variable Parameter Algorithm (STVP)
  
Until now, the WAR minimization method provided a means to analyze the price reactions to the news over an entire sample period. In practice, it may be more useful to have a self-updating method that iteratively uses every new observation to improve the estimation progressively. It can also be argued that the emphasis of the prediction should be placed in gauging the shock that could occur in the present. When building this model, the assumption is that information available at time $t$ includes everything before that moment, specifically information set $\Omega_{t-1}$ This is in contrast with the WAR minimization and is why this particular method is also included.
  
Starting once more from the baseline case introduced in section [Section 2](#bb), a time-dependent measure of market reaction to news $\beta_t$ can be constructed by 1.) Transforming the problem of Ordinary Least Squares so that it can be recursively solved and 2.) Allowing for the parameter(s) to change over time sequentially. Therefore, in sequence the
  
To obtain the STVP algorithm presented in [Section 9.3](#stepsstvp) the first step consists of replacing the OLS equations by their recursive counterparts and achieve the intermediate SRLS algorithm. The following equations are similar to the familiar OLS equations setup.

\begin{equation}
R_t = S_t^{T}\beta_1 + e_t
\end{equation}
  
The error vector contains a series of random variables $e_t$ that are assumed to be normally distributed with 0 mean and serially uncorrelated. We omit the constant intercept; the reasoning is elaborated in [Section 9.3](#stepswar).
  
\begin{equation}
\hat{\beta}_{OLS} = (S^{T}S)^{-1}S^{T}R 
\end{equation}
  
\begin{equation}
\beta^{OLS}_t = \beta^{OLS}_{t-1} \forall t
\end{equation}
  
When an observation $y_j$ is added to the samples $i = 1,...,N$, an update to the $\hat{\beta}$ is the same as re-estimating the entire sample with the new observation. Effectively, the "intermediate model" or Stochastic Recursive Least Squares Algorithm (SRLS) is thereafter obtained (the exact step-by-step is in [Section 9.3](#stepsslrs)). While this is a powerful tool, it does not necessarily help estimate $y_j$ effectively. It is giving just as much weight to realizations of $y$ early in the sample than those that immediately precede the new extra observation $j$. The contribution of every marginal observation is smaller and smaller as the sample grows. The $\hat{\beta}$ converges to the full sample parameter estimator $\hat{\beta}_{OLS}$ as seen in Figure  \@ref(fig:slrsplot).
  
```{r slrsplot, echo = FALSE, fig.cap="SRLS applied to the UK CPI"}
plotslrs(read.csv("~/R tests/finance related projects/slrs_df.csv"))
```
  
Additional data points in the aformentioned methodology add information to the model and improve the accuracy of the $\hat{\beta}$ estimator but it is always assumed that there is only one true $\beta$ regardless of time $t$. In order to attenuate this assumption and include the flexibility that allows for a time dependence in a $\beta_t$, we introduce the random walk disturbance term mentioned in [Section 2](#grw). The error term is now characterized by the incremental steps of a random walk. In bayesian terms, the previous estimation of $\beta_t$ using the random walk becomes the *prior*. 
  
\begin{equation}
R_t = S_t^{T}\beta_t + e_t \quad e_t \sim \mathcal{N}(0,\sigma^2)
\end{equation}
  
\begin{equation}
\hat{\beta}_t = E(\breve{\beta}_t|\hat{\beta}_{t-1})
\end{equation}
  
\begin{equation}
\beta_t = \beta_{t-1} + \eta_{t-1} \quad \eta_t \sim \mathcal{N}(0,\sigma^2_{\beta})
\end{equation}
  
The equations above are thus the foundation for the construction of the algorithm that is implemented for use and shown in [Section 9.3](#stepsstvp). When initializing the first values, we set the variance of the estimator $P^*_t$ to be very large and the $\beta_t$ parameter to be 0 to reflect 0 knowledge of the nature of the estimation at the beginning of the sample. 
    
```{r stvpplot, echo = FALSE, fig.cap="STVP applied to the UK CPI"}
plotslrs(read.csv("~/R tests/finance related projects/stvp_df.csv"))
```

# Discussion
  
## Differences and Irregularities of Two Paths
  
It is quite evident the STVP path is "slower to react" to the changes of its parameter. Informally, the reason for the lag is simply that the method does not benefit from retrospective analysis and is using only previous information to make an estimation. The high credible intervals towards the beginning of the sample are conveying the uncertainty.
  
The WAR Estimation method is especially useful when trying to analyze the evolution of the parameter over the entire sample period. Because of the "forward and backward passes" in the algorithm seen in steps (b) and (c) of section [Section 9.4](#stepswar), the beginning of the sample period is estimated with as much precision as its ending. The addition of a reverse order filter means that the resulting estimate is "smoothed over" twice. This is a positive feature because there may be occasionally be "noisy" erronous data points or exceptions due to outside factors that are "smoothed" out by the dictating trend and nearby observations both before and after their occurence. Moreover, there is an inherent restriction of $\sum^T_t \delta_t = 0$, meaning that the positive $\delta_t$ deviations from the average parameter value or$\beta_{OLS}$ are exactly counterpoised by the negative ones. The path will always have the original, stable, time-invariant estimator as an average.   

On the contrary, the STVP estimation is susceptible to exceptional or isolated changes to the price-reaction to the macro-announcements at any time during the sample. The $\beta_t$ path can drastically change when both the surprise $S_t$ shock and the difference $R_t - S_t^T\beta_{t-1}$ are large in absolute terms. A higher noise variance $\hat{\sigma}^2$ (constructed using residual errors) reduces the amount of variation the parameter will take while increasing the distance of the bounds which is undesirable but reflects the amount of uncertainty there is. 

## Accuracy of the Estimations

Using the bayesian perspective, the accuracy of the WAR line is measured using 95% posterior probability intervals (with priors being the weighting function of the random walks each having one of the 11 end-point variances.) [@muller_efficient_2010]. Therefore, given that the prior is correct and representative of the stable scenario case, the $\beta_t$ lies within the interval zone with a 95% probability. The posterior distribution at each time $t$, is a weighted compromise of the different random walks and is the variance of this distribution that is used for the credible intervals.
  
Conversely, the STVP estimation at time $t$ is achieved using the previous observation at $t-1$ as a prior. One can observe that the STVP estimation bounds are typically much more narrow than that of the WAR. 
    
## One-Step ahead Forecasts
  
While the primary aim of the study was not obtain forecasts, we investigate the possibilty of a naive prediction with their associated prediction intervals. The term naive is used because it will be assumed that the best measures for expectation and variance for the 5 minute pip change of $\hat{R}_{t+1}$ are obtained by using the most recent estimates of the sample at time $t$ . Furthermore, for this forecast, we put ourselves in the situation of an investor who has just observed the release of the UK CPI on January 15th 2020 at 11:30:00, and would like a measurement of certainty of where price will lie 5 minutes afterwards.

Formally, the conditional expectation and variance 
  
## Use of the Post-Announcement 5 Minute Window
  
The erronous or exceptional data can sometimes be quite obvious. An example would be that of the Canadian Core Retail Sales in July of 2013. On this day, the figure achieved a 1.2% increase versus the 0.1% that was expected and yet the 5 minute post-price difference was merely -11.3 pips. The price move was instantaneous to the extent that the beginning of the 14:30:00 candle already marked the majority of the price shock. A solution to this issue would have been to consider using a price before 14:30:00 although this would have introduced the issue of finding a general timing for all other releases that does not include irrelevant price movements before to the announcement time-stamp. Furthermore, in some circumstances, it could be desirable to omit these price jumps as they would result in what is called "slippage" (A difference between the price an investor will place an order at and the price it is executed at).
  
![USD/CAD on the 27th of July between 14h00 and 15h00 GMT+2. The "missing" data can be attributed to the sudden change in price and lack of liquidity in the moments preceding 14:30:00.](candlestick3.png)


  

There are several explanations for these exceptions. One example in particular would be market interventions of the central banks which have been proven to occur during or in time proximity to figure releases as shown in studies such as that of @dominguez_market_2003. Similarly, there is evidence of insider trading preceding news announcements. Assets traded in such as the E-mini S&P 500 futures where exact order imbalance can be measured. It is perfectly possible that the same kind of activity is taking place in the decentralized currency exchange (or FX) markets [@bernile_can_2016]. It is impossible (or extremely difficult) to detect these external reasons for price shocks and the WAR conveniently "smooths" them over.

One can observe that the estimation can "stray away from the dictating trend" to a larger extent than the other method. 


# Appendix

## Parameter Paths
  
The following path estimations should be interpreted as follows: a one standard deviation unanticipated positive change in the economic variable $S_t$ results in an appreciation or depreciation in the exchange rate of the relevant currency pair by $R_t$ amount of pips or basis points. 
  
```{r cadcrspath, echo = FALSE, fig.cap = "Note: 1 Std Dev is +0.605% greater than expected. Currency pair: USD/CAD."}
plotpath2(read.csv("~/R tests/finance related projects/cadcrs_path.csv", row.names = 1),
          read.csv("~/R tests/finance related projects/cadcrs_path_stvp.csv", row.names = 1),
          "Canadian Core Retail Sales")
```
  
```{r cadcpipath, echo = FALSE, fig.cap = "Note: 1 Std Dev is +0.229% greater than expected. Currency pair: USD/CAD."}
plotpath2(read.csv("~/R tests/finance related projects/cadcpi_path.csv", row.names = 1),
         read.csv("~/R tests/finance related projects/cadcpi_path_stvp.csv", row.names = 1),"Canadian Consumer Price Index")
```
  
```{r audretpath, echo = FALSE, fig.cap = "Note: 1 Std Dev is +0.569% greater than expected. Currency pair: AUD/USD."}
plotpath2(read.csv("~/R tests/finance related projects/audret_path.csv", row.names = 1),
          read.csv("~/R tests/finance related projects/audret_path_stvp.csv", row.names = 1),
          "Australian Retail Sales")
```
  
```{r audcpipath, echo = FALSE, fig.cap = "Note: 1 Std Dev is +0.229% greater than expected. Currency pair: AUD/USD."}
plotpath2(read.csv("~/R tests/finance related projects/audcpi_path.csv", row.names = 1), 
          read.csv("~/R tests/finance related projects/audcpi_path_stvp.csv", row.names = 1), 
          "Australian Consumer Price Index")
```
  
```{r usdcpipath, echo = FALSE, fig.cap = "Note: 1 Std Dev is +0.117% greater than expected. Currency pair: USD/CHF."}
plotpath2(read.csv("~/R tests/finance related projects/usdcpi_path.csv", row.names = 1),
          read.csv("~/R tests/finance related projects/usdcpi_path_stvp.csv", row.names = 1),
          "US Consumer Price Index")
```
  
```{r gbpcpipath, echo = FALSE, fig.cap = "Note: 1 Std Dev is +0.173% greater than expected. Currency pair: GBP/USD."}
plotpath2(read.csv("~/R tests/finance related projects/gbpcpi_path.csv", row.names = 1),
          read.csv("~/R tests/finance related projects/gbpcpi_path_stvp.csv",row.names = 1),
          "UK Consumer Price Index")
```
  
```{r nzdcpipath, echo = FALSE, fig.cap = "Note: 1 Std Dev is +0.206% greater than expected. Currency pair: NZD/USD."}
plotpath2(read.csv("~/R tests/finance related projects/nzdcpi_path.csv", row.names = 1),
          read.csv("~/R tests/finance related projects/nzdcpi_path_stvp.csv", row.names = 1),
          "New Zealand Consumer Price Index")
```
  
```{r usdnfppath, echo = FALSE, fig.cap = "Note: 1 Std Dev is +66.5 thousand more people than expected. Currency pair: USDCHF."}
plotpath2(read.csv("~/R tests/finance related projects/usdnfp_path.csv", row.names = 1),
          read.csv("~/R tests/finance related projects/usdnfp_path_stvp.csv", row.names = 1),
          "US Non-Farm Payrolls")
```
  
## Stochastic Recursive Least Squares Algorithm (SRLS) {#stepsslrs}
  
1. $\hat{\alpha}_t = \hat{\alpha}_{t-1} + g_t(R_t - S^T_t\hat{\alpha}_{t-1})$
2. $g_t = P^*_{t-1}S_t(\hat{\sigma}^2 + S^T_tP^*_{t-1}S_t)^{-1}$
3. $P^*_t = P^*_{t-1} - g_tS^T_tP^*_{t-1}$
  
## Standard Recursive Time Variable Parameter Algorithm (STVP) {#stepsstvp}
  
In the case of this study, it is assumed that the parameter follows a simple random walk. As a result, $A = D = I$. A prior value that represents a 25% change per observation is chosen for the diagonals of $Q_a$.  
  
Prediction (Prior)
   
1. $\hat{\alpha}_t|\hat{\alpha}_{t-1} = A\alpha_{t-1}$  
2. $P^*_t|P^*_{t-1} = AP^*_{t-1}A^T+DQ_aD^T$
  
Correction (Posterior)
  
3. $\hat{\alpha}_t = \hat{\alpha}_t|\hat{\alpha}_{t-1} + g_t(R_t - S^T_t(\hat{\alpha}_t|\hat{\alpha}_{t-1}))$  
4. $g_t = (P^*_t|P^*_{t-1})S_t(\hat{\sigma}^2 + S^T_t(P^*_t|P^*_{t-1}))^{-1}$
5. $P^*_t = P^*_t|P^*_{t-1} - g_tS^T_t(P^*_t|P^*_{t-1})$
  
## Steps to obtain qLL statistic {#stepsqLL}
  
1. Compute  the OLS residuals $\hat{\varepsilon}_t$ by regressing $R_t$ on ${S_t, Z_t}$;  
2. Construct a consistent estimator $\hat{V}_X$ of the $k*k$ long-run covariance matrix of $S_t\varepsilon_t$. When $\varepsilon_t$ can be assumed uncorrelated, a natural choice is the heteroscedasticity robust estimator $\hat{V}_X = T^{-1}\sum_{t=1}^TX_tX_t'\varepsilon_t^2$  
3. Compute $\hat{U}_t = \hat{V}_X^{-1/2}X_t\hat{\varepsilon}_t$ and denote the k elements of $\hat{U}_t$ by $\hat{U}_{t,i}$, $i = 1,...,k$.  
4. For each series $\hat{U}_{t,i}$, compute a new series, $\hat{w}_{t,i}$ via ${w}_{t,i} = \bar{r}\hat{w}_{t-1,i} + \Delta\hat{U}_{t,i}$, and $\hat{w}_{1,i} = \hat{U}_{1,i}$, where $\bar{r} = 1 - 10/T$.  
5. Compute the squared residuals from OLS regressions of $\hat{w}_{t,i}$ on $\bar{r}^t$ individually, and sum all of those over $i = 1,...,k$.  
6. Multiply this sum of sum of squared residuals by $\bar{r}$, and subtract $\sum_{i=1}^k \sum_{t=1}^T (\hat{U}_{t,i})^2$
  
## Steps to obtain WAR minimization path {#stepswar}
  
1. For t = 1,...,T, let $a_t$ and $b_t$ be the first $p$ elements of $\hat{H}^{-1}s_t(\hat{\theta})$ and $\hat{H}\hat{V}^{-1}s_t(\hat{\theta})$ respectively.  
2. For $c_i \in C = {0, 5, 10,...,50}, i = 1,...,11$ compute
  (a) $r_i = 1-c_i/T$, $z_{i,1} = x_1$ and $z_{i,t} = r_{i}z_{i,t-1} + x_t - x_{t-1}, t = 2,...,T$;  
  (b) the residuals $\{\tilde{z}_{i,t}\}_{t=1}^T$ of a linear regression of $\{{z}_{i,t}\}_{t=1}^T$ on $\{r_i^{t-1}I_p\}_{t=1}^T$  
  (c) $\bar{z}_{i,T} = \tilde{z}_{i,T}$, and $\bar{z}_{i,t} = r_i\bar{z}_{i,t+1} + \tilde{z}_{i,t} - \tilde{z}_{i,t+1}, t = 1,...,T-1$;  
  (d) $\{\hat{\beta}_{i,t}\}_{t=1}^T = \{\hat{\theta}\ + a_t - r_i \bar{z}_{i,t}\}_{t=1}^T$;  
  (e) $qLL(c_i) = \sum_{t=1}^T(r_i)\bar{z}_{i,t} - a_t)'\tilde{b}_{t}$ and $\tilde{w}_{i} = \sqrt{T(1-r_i^2)r_i^{T-1}/(1-r_i^{2T})}e^{-\frac{1}{2}qLL(c_i)}$ (set $\tilde{w}_{0} = 1$)
3. Compute $w_i = \tilde{w_i}/ \sum_{j=1}^{11}\tilde{w_j}$.  
4. The parameter path estimator is given by ${\{\hat{\beta}_{t}\}_{t=1}^T} = \{\sum_{i=1}^{11}w_i\hat{\beta}_{i,t}\}_{t=1}^T$.  
5. The statistic qLL(10) tests the null hypothesis of stability of $\beta$ and rejects for small values. Critical values depend on $p$ and are tabulated in Table 1 of @elliott_efficient_2006 and \@ref(tab:qLL).  

## Variance for credible intervals for the WAR path
  
$\Omega_t = \sum^{10}_{i=1}w_i(T^{-1}\hat{S}_\beta\kappa_t(c_i) + (\hat{\beta}_{i,t} - \hat{\beta_t})(\hat{\beta}_{i,t} - \hat{\beta_t})')$, $\kappa_t(c) = \frac{c(1 + e^{2c} + e^{2ct/T} + e^2c(1-t/T))}{2e^{2c} - 2}$
  
# References
